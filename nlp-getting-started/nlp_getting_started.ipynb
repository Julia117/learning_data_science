{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "damaged-monitoring",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "vocational-regulation",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.lang.en import English\n",
    "import contextualSpellCheck\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from string import punctuation\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "capital-starter",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('nlp-getting-started/train.csv', index_col=0)\n",
    "test_data = pd.read_csv('nlp-getting-started/test.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "civil-joining",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   keyword location                                               text  target\n",
       "id                                                                            \n",
       "1      NaN      NaN  Our Deeds are the Reason of this #earthquake M...       1\n",
       "4      NaN      NaN             Forest fire near La Ronge Sask. Canada       1\n",
       "5      NaN      NaN  All residents asked to 'shelter in place' are ...       1\n",
       "6      NaN      NaN  13,000 people receive #wildfires evacuation or...       1\n",
       "7      NaN      NaN  Just got sent this photo from Ruby #Alaska as ...       1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ultimate-solid",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7613, 4) (3263, 3)\n"
     ]
    }
   ],
   "source": [
    "print(train_data.shape, test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "revised-aviation",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "keyword       61\n",
       "location    2533\n",
       "text           0\n",
       "target         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "killing-savannah",
   "metadata": {},
   "source": [
    "## Dealing with mislabeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "brazilian-spoke",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['like for the music video I want some real action shit like burning buildings and police chases not some weak ben winston shit',\n",
       " 'Hellfire! We don\\x89Ûªt even want to think about it or mention it so let\\x89Ûªs not do anything that leads to it #islam!',\n",
       " \"The Prophet (peace be upon him) said 'Save yourself from Hellfire even if it is by giving half a date in charity.'\",\n",
       " 'In #islam saving a person is equal in reward to saving all humans! Islam is the opposite of terrorism!',\n",
       " 'To fight bioterrorism sir.',\n",
       " 'Who is bringing the tornadoes and floods. Who is bringing the climate change. God is after America He is plaguing her\\n \\n#FARRAKHAN #QUOTE',\n",
       " '#foodscare #offers2go #NestleIndia slips into loss after #Magginoodle #ban unsafe and hazardous for #humanconsumption',\n",
       " '#Allah describes piling up #wealth thinking it would last #forever as the description of the people of #Hellfire in Surah Humaza. #Reflect',\n",
       " 'He came to a land which was engulfed in tribal war and turned it into a land of peace i.e. Madinah. #ProphetMuhammad #islam',\n",
       " 'RT NotExplained: The only known image of infamous hijacker D.B. Cooper. http://t.co/JlzK2HdeTG',\n",
       " 'Hellfire is surrounded by desires so be careful and don\\x89Ûªt let your desires control you! #Afterlife',\n",
       " 'CLEARED:incident with injury:I-495  inner loop Exit 31 - MD 97/Georgia Ave Silver Spring',\n",
       " \"Mmmmmm I'm burning.... I'm burning buildings I'm building.... Oooooohhhh oooh ooh...\",\n",
       " 'wowo--=== 12000 Nigerian refugees repatriated from Cameroon',\n",
       " '.POTUS #StrategicPatience is a strategy for #Genocide; refugees; IDP Internally displaced people; horror; etc. https://t.co/rqWuoy1fm4',\n",
       " 'Caution: breathing may be hazardous to your health.',\n",
       " 'I Pledge Allegiance To The P.O.P.E. And The Burning Buildings of Epic City. ??????',\n",
       " 'that horrible sinking feeling when you\\x89Ûªve been at home on your phone for a while and you realise its been on 3G this whole time']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mislabeled_data = train_data.groupby(['text']).nunique().sort_values(by='target', ascending=False)\n",
    "mislabeled_data = mislabeled_data[mislabeled_data['target'] > 1]['target']\n",
    "mislabeled_data.index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "alleged-tender",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_target(l):\n",
    "    true_labels = [0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0]\n",
    "    for i in range(len(l)):\n",
    "        indexes = train_data.loc[train_data['text'] == l[i]].index\n",
    "        for index in indexes:\n",
    "            train_data.loc[index, 'target'] = true_labels[i]           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "posted-ranch",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "l = mislabeled_data.index.tolist()\n",
    "fix_target(l)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recognized-effort",
   "metadata": {},
   "source": [
    "## Separate Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "individual-agent",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = pd.concat([train_data.text, test_data.text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "breeding-ancient",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id\n",
       "1    Our Deeds are the Reason of this #earthquake M...\n",
       "4               Forest fire near La Ronge Sask. Canada\n",
       "5    All residents asked to 'shelter in place' are ...\n",
       "6    13,000 people receive #wildfires evacuation or...\n",
       "7    Just got sent this photo from Ruby #Alaska as ...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "freelance-wheat",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = English()  # Includes English data\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "lemmatizer = nlp.get_pipe(\"lemmatizer\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alpha-place",
   "metadata": {},
   "source": [
    "## Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "human-promotion",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from string import punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "tight-night",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_username(text):\n",
    "    pattern = '@[/w]*'\n",
    "    return [word for word in text if not re.match(pattern, word.text)] \n",
    "\n",
    "def remove_links(text):\n",
    "    pattern = \"(https?:\\/\\/)?([\\da-z\\.-]+)\\.([a-z\\.]{2,6})([\\/\\w \\.-]*)\"\n",
    "    return [word for word in text if not re.match(pattern, word.text)] \n",
    "\n",
    "def text_to_token(text):\n",
    "    return nlp(text)\n",
    "\n",
    "def remove_digits(text):\n",
    "    pattern = '^[0-9]*([,.][0-9]*)?$'\n",
    "    return [word for word in text if not re.match(pattern, word.text)] \n",
    "\n",
    "def remove_stopwords(text):\n",
    "    stopwords = nlp.Defaults.stop_words\n",
    "    return [word for word in text if word not in stopwords] \n",
    "\n",
    "def remove_short_words(text):\n",
    "    return [word for word in text if len(word) > 3]\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    return [word for word in text if word.text not in punctuation and word.text not in ['\\n']]\n",
    "\n",
    "def lemmatize(text):\n",
    "    return[token.lemma_ for token in text]\n",
    "\n",
    "def list_to_str(l):\n",
    "    return ' '.join(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "registered-minutes",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextTokenizerTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self ):\n",
    "        print(\"Tokenize\")\n",
    "\n",
    "    def fit(self, X):\n",
    "        return self    \n",
    "        \n",
    "    def transform(self, X):\n",
    "        text = X.apply(text_to_token)\n",
    "        return text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "cheap-baghdad",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextCleanerTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self ):\n",
    "        print(\"Clean text\")\n",
    "\n",
    "    def fit(self, X):\n",
    "        return self    \n",
    "        \n",
    "    def transform(self, X):\n",
    "        text = X.apply(remove_digits)\n",
    "        text = text.apply(remove_stopwords)\n",
    "        text = text.apply(remove_punctuation)\n",
    "        text = text.apply(remove_username)\n",
    "        text = text.apply(remove_links)\n",
    "        text = text.apply(remove_short_words)\n",
    "        return text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "humanitarian-recorder",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextLemmatizerTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self ):\n",
    "        print(\"Lemmatize\")\n",
    "\n",
    "    def fit(self, X):\n",
    "        return self    \n",
    "        \n",
    "    def transform(self, X):\n",
    "        lemmatized_text = X.apply(lemmatize)\n",
    "        lemmatized_text = lemmatized_text.apply(remove_stopwords)\n",
    "        text = lemmatized_text.apply(list_to_str)\n",
    "        return text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "breeding-intensity",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenize\n",
      "Clean text\n",
      "Lemmatize\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "pipe = Pipeline([\n",
    "    ('tokenize', TextTokenizerTransformer()),\n",
    "    ('clean', TextCleanerTransformer()),\n",
    "    ('lemmatize', TextLemmatizerTransformer())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "blank-truck",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = pipe.fit_transform(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "animated-article",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'deed Reason earthquake ALLAH forgive'"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "welsh-boost",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "hybrid-simulation",
   "metadata": {},
   "source": [
    "### Spelling correction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "exempt-nebraska",
   "metadata": {},
   "outputs": [],
   "source": [
    "from autocorrect import Speller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "noble-african",
   "metadata": {},
   "outputs": [],
   "source": [
    "spell = Speller()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "characteristic-character",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import contextualSpellCheck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "federal-component",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "Income was $9.4 million compared to the prior year of $2.7 million.\n"
     ]
    }
   ],
   "source": [
    "# import spacy\n",
    "# nlp = spacy.load('en_core_web_sm')\n",
    "# contextualSpellCheck.add_to_pipe(nlp)\n",
    "# doc = nlp('Income was $9.4 milion compared to the prior year of $2.7 milion.')\n",
    "\n",
    "# print(doc._.performed_spellCheck) #Should be True\n",
    "# print(doc._.outcome_spellCheck) #Income was $9.4 million compared to the prior year of $2.7 million."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "contrary-hazard",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<spacy.pipeline.sentencizer.Sentencizer at 0x7f8e83db76e0>"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# nlp.add_pipe('sentencizer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "popular-washington",
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_spelling(text):\n",
    "    return [spell(word) for word in text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "guilty-credits",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id\n",
       "170    [I, seeing, Issues, aftershock]\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[117:118].apply(correct_spelling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "understanding-option",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "100\n",
      "110\n",
      "120\n",
      "130\n",
      "140\n",
      "150\n",
      "160\n",
      "170\n",
      "180\n",
      "190\n",
      "200\n",
      "210\n",
      "220\n",
      "230\n",
      "240\n",
      "250\n",
      "260\n",
      "270\n",
      "280\n",
      "290\n",
      "300\n",
      "310\n",
      "320\n",
      "330\n",
      "340\n",
      "350\n",
      "360\n",
      "370\n",
      "380\n",
      "390\n",
      "400\n",
      "410\n",
      "420\n",
      "430\n",
      "440\n",
      "450\n",
      "460\n",
      "470\n",
      "480\n",
      "490\n",
      "500\n",
      "510\n",
      "520\n",
      "530\n",
      "540\n",
      "550\n",
      "560\n",
      "570\n",
      "580\n",
      "590\n",
      "600\n",
      "610\n",
      "620\n",
      "630\n",
      "640\n",
      "650\n",
      "660\n",
      "670\n",
      "680\n",
      "690\n",
      "700\n",
      "710\n",
      "720\n",
      "730\n",
      "740\n",
      "750\n",
      "760\n",
      "770\n",
      "780\n",
      "790\n",
      "800\n",
      "810\n",
      "820\n",
      "830\n",
      "840\n",
      "850\n",
      "860\n",
      "870\n",
      "880\n",
      "890\n",
      "900\n",
      "910\n",
      "920\n",
      "930\n",
      "940\n",
      "950\n",
      "960\n",
      "970\n",
      "980\n",
      "990\n",
      "1000\n",
      "1010\n",
      "1020\n",
      "1030\n",
      "1040\n",
      "1050\n",
      "1060\n",
      "1070\n",
      "1080\n",
      "1090\n",
      "1100\n",
      "1110\n",
      "1120\n",
      "1130\n",
      "1140\n",
      "1150\n",
      "1160\n",
      "1170\n",
      "1180\n",
      "1190\n",
      "1200\n",
      "1210\n",
      "1220\n",
      "1230\n",
      "1240\n",
      "1250\n",
      "1260\n",
      "1270\n",
      "1280\n",
      "1290\n",
      "1300\n",
      "1310\n",
      "1320\n",
      "1330\n",
      "1340\n",
      "1350\n",
      "1360\n",
      "1370\n",
      "1380\n",
      "1390\n",
      "1400\n",
      "1410\n",
      "1420\n",
      "1430\n",
      "1440\n",
      "1450\n",
      "1460\n",
      "1470\n",
      "1480\n",
      "1490\n",
      "1500\n",
      "1510\n",
      "1520\n",
      "1530\n",
      "1540\n",
      "1550\n",
      "1560\n",
      "1570\n",
      "1580\n",
      "1590\n",
      "1600\n",
      "1610\n",
      "1620\n",
      "1630\n",
      "1640\n",
      "1650\n",
      "1660\n",
      "1670\n",
      "1680\n",
      "1690\n",
      "1700\n",
      "1710\n",
      "1720\n",
      "1730\n",
      "1740\n",
      "1750\n",
      "1760\n",
      "1770\n",
      "1780\n",
      "1790\n",
      "1800\n",
      "1810\n",
      "1820\n",
      "1830\n",
      "1840\n",
      "1850\n",
      "1860\n",
      "1870\n",
      "1880\n",
      "1890\n",
      "1900\n",
      "1910\n",
      "1920\n",
      "1930\n",
      "1940\n",
      "1950\n",
      "1960\n",
      "1970\n",
      "1980\n",
      "1990\n",
      "2000\n",
      "2010\n",
      "2020\n",
      "2030\n",
      "2040\n",
      "2050\n",
      "2060\n",
      "2070\n",
      "2080\n",
      "2090\n",
      "2100\n",
      "2110\n",
      "2120\n",
      "2130\n",
      "2140\n",
      "2150\n",
      "2160\n",
      "2170\n",
      "2180\n",
      "2190\n",
      "2200\n",
      "2210\n",
      "2220\n",
      "2230\n",
      "2240\n",
      "2250\n",
      "2260\n",
      "2270\n",
      "2280\n",
      "2290\n",
      "2300\n",
      "2310\n",
      "2320\n",
      "2330\n",
      "2340\n",
      "2350\n",
      "2360\n",
      "2370\n",
      "2380\n",
      "2390\n",
      "2400\n",
      "2410\n",
      "2420\n",
      "2430\n",
      "2440\n",
      "2450\n",
      "2460\n",
      "2470\n",
      "2480\n",
      "2490\n",
      "2500\n",
      "2510\n",
      "2520\n",
      "2530\n",
      "2540\n",
      "2550\n",
      "2560\n",
      "2570\n",
      "2580\n",
      "2590\n",
      "2600\n",
      "2610\n",
      "2620\n",
      "2630\n",
      "2640\n",
      "2650\n",
      "2660\n",
      "2670\n",
      "2680\n",
      "2690\n",
      "2700\n",
      "2710\n",
      "2720\n",
      "2730\n",
      "2740\n",
      "2750\n",
      "2760\n",
      "2770\n",
      "2780\n",
      "2790\n",
      "2800\n",
      "2810\n",
      "2820\n",
      "2830\n",
      "2840\n",
      "2850\n",
      "2860\n",
      "2870\n",
      "2880\n",
      "2890\n",
      "2900\n",
      "2910\n",
      "2920\n",
      "2930\n",
      "2940\n",
      "2950\n",
      "2960\n",
      "2970\n",
      "2980\n",
      "2990\n",
      "3000\n",
      "3010\n",
      "3020\n",
      "3030\n",
      "3040\n",
      "3050\n",
      "3060\n",
      "3070\n",
      "3080\n",
      "3090\n",
      "3100\n",
      "3110\n",
      "3120\n",
      "3130\n",
      "3140\n",
      "3150\n",
      "3160\n",
      "3170\n",
      "3180\n",
      "3190\n",
      "3200\n",
      "3210\n",
      "3220\n",
      "3230\n",
      "3240\n",
      "3250\n",
      "3260\n",
      "3270\n",
      "3280\n",
      "3290\n",
      "3300\n",
      "3310\n",
      "3320\n",
      "3330\n",
      "3340\n",
      "3350\n",
      "3360\n",
      "3370\n",
      "3380\n",
      "3390\n",
      "3400\n",
      "3410\n",
      "3420\n",
      "3430\n",
      "3440\n",
      "3450\n",
      "3460\n",
      "3470\n",
      "3480\n",
      "3490\n",
      "3500\n",
      "3510\n",
      "3520\n",
      "3530\n",
      "3540\n",
      "3550\n",
      "3560\n",
      "3570\n",
      "3580\n",
      "3590\n",
      "3600\n",
      "3610\n",
      "3620\n",
      "3630\n",
      "3640\n",
      "3650\n",
      "3660\n",
      "3670\n",
      "3680\n",
      "3690\n",
      "3700\n",
      "3710\n",
      "3720\n",
      "3730\n",
      "3740\n",
      "3750\n",
      "3760\n",
      "3770\n",
      "3780\n",
      "3790\n",
      "3800\n",
      "3810\n",
      "3820\n",
      "3830\n",
      "3840\n",
      "3850\n",
      "3860\n",
      "3870\n",
      "3880\n",
      "3890\n",
      "3900\n",
      "3910\n",
      "3920\n",
      "3930\n",
      "3940\n",
      "3950\n",
      "3960\n",
      "3970\n",
      "3980\n",
      "3990\n",
      "4000\n",
      "4010\n",
      "4020\n",
      "4030\n",
      "4040\n",
      "4050\n",
      "4060\n",
      "4070\n",
      "4080\n",
      "4090\n",
      "4100\n",
      "4110\n",
      "4120\n",
      "4130\n",
      "4140\n",
      "4150\n",
      "4160\n",
      "4170\n",
      "4180\n",
      "4190\n",
      "4200\n",
      "4210\n",
      "4220\n",
      "4230\n",
      "4240\n",
      "4250\n",
      "4260\n",
      "4270\n",
      "4280\n",
      "4290\n",
      "4300\n",
      "4310\n",
      "4320\n",
      "4330\n",
      "4340\n",
      "4350\n",
      "4360\n",
      "4370\n",
      "4380\n",
      "4390\n",
      "4400\n",
      "4410\n",
      "4420\n",
      "4430\n",
      "4440\n",
      "4450\n",
      "4460\n",
      "4470\n",
      "4480\n",
      "4490\n",
      "4500\n",
      "4510\n",
      "4520\n",
      "4530\n",
      "4540\n",
      "4550\n",
      "4560\n",
      "4570\n",
      "4580\n",
      "4590\n",
      "4600\n",
      "4610\n",
      "4620\n",
      "4630\n",
      "4640\n",
      "4650\n",
      "4660\n",
      "4670\n",
      "4680\n",
      "4690\n",
      "4700\n",
      "4710\n",
      "4720\n",
      "4730\n",
      "4740\n",
      "4750\n",
      "4760\n",
      "4770\n",
      "4780\n",
      "4790\n",
      "4800\n",
      "4810\n",
      "4820\n",
      "4830\n",
      "4840\n",
      "4850\n",
      "4860\n",
      "4870\n",
      "4880\n",
      "4890\n",
      "4900\n",
      "4910\n",
      "4920\n",
      "4930\n",
      "4940\n",
      "4950\n",
      "4960\n",
      "4970\n",
      "4980\n",
      "4990\n",
      "5000\n",
      "5010\n",
      "5020\n",
      "5030\n",
      "5040\n",
      "5050\n",
      "5060\n",
      "5070\n",
      "5080\n",
      "5090\n",
      "5100\n",
      "5110\n",
      "5120\n",
      "5130\n",
      "5140\n",
      "5150\n",
      "5160\n",
      "5170\n",
      "5180\n",
      "5190\n",
      "5200\n",
      "5210\n",
      "5220\n",
      "5230\n",
      "5240\n",
      "5250\n",
      "5260\n",
      "5270\n",
      "5280\n",
      "5290\n",
      "5300\n",
      "5310\n",
      "5320\n",
      "5330\n",
      "5340\n",
      "5350\n",
      "5360\n",
      "5370\n",
      "5380\n",
      "5390\n",
      "5400\n",
      "5410\n",
      "5420\n",
      "5430\n",
      "5440\n",
      "5450\n",
      "5460\n",
      "5470\n",
      "5480\n",
      "5490\n",
      "5500\n",
      "5510\n",
      "5520\n",
      "5530\n",
      "5540\n",
      "5550\n",
      "5560\n",
      "5570\n",
      "5580\n",
      "5590\n",
      "5600\n",
      "5610\n",
      "5620\n",
      "5630\n",
      "5640\n",
      "5650\n",
      "5660\n",
      "5670\n",
      "5680\n",
      "5690\n",
      "5700\n",
      "5710\n",
      "5720\n",
      "5730\n",
      "5740\n",
      "5750\n",
      "5760\n",
      "5770\n",
      "5780\n",
      "5790\n",
      "5800\n",
      "5810\n",
      "5820\n",
      "5830\n",
      "5840\n",
      "5850\n",
      "5860\n",
      "5870\n",
      "5880\n",
      "5890\n",
      "5900\n",
      "5910\n",
      "5920\n",
      "5930\n",
      "5940\n",
      "5950\n",
      "5960\n",
      "5970\n",
      "5980\n",
      "5990\n",
      "6000\n",
      "6010\n",
      "6020\n",
      "6030\n",
      "6040\n",
      "6050\n",
      "6060\n",
      "6070\n",
      "6080\n",
      "6090\n",
      "6100\n",
      "6110\n",
      "6120\n",
      "6130\n",
      "6140\n",
      "6150\n",
      "6160\n",
      "6170\n",
      "6180\n",
      "6190\n",
      "6200\n",
      "6210\n",
      "6220\n",
      "6230\n",
      "6240\n",
      "6250\n",
      "6260\n",
      "6270\n",
      "6280\n",
      "6290\n",
      "6300\n",
      "6310\n",
      "6320\n",
      "6330\n",
      "6340\n",
      "6350\n",
      "6360\n",
      "6370\n",
      "6380\n",
      "6390\n",
      "6400\n",
      "6410\n",
      "6420\n",
      "6430\n",
      "6440\n",
      "6450\n",
      "6460\n",
      "6470\n",
      "6480\n",
      "6490\n",
      "6500\n",
      "6510\n",
      "6520\n",
      "6530\n",
      "6540\n",
      "6550\n",
      "6560\n",
      "6570\n",
      "6580\n",
      "6590\n",
      "6600\n",
      "6610\n",
      "6620\n",
      "6630\n",
      "6640\n",
      "6650\n",
      "6660\n",
      "6670\n",
      "6680\n",
      "6690\n",
      "6700\n",
      "6710\n",
      "6720\n",
      "6730\n",
      "6740\n",
      "6750\n",
      "6760\n",
      "6770\n",
      "6780\n",
      "6790\n",
      "6800\n",
      "6810\n",
      "6820\n",
      "6830\n",
      "6840\n",
      "6850\n",
      "6860\n",
      "6870\n",
      "6880\n",
      "6890\n",
      "6900\n",
      "6910\n",
      "6920\n",
      "6930\n",
      "6940\n",
      "6950\n",
      "6960\n",
      "6970\n",
      "6980\n",
      "6990\n",
      "7000\n",
      "7010\n",
      "7020\n",
      "7030\n",
      "7040\n",
      "7050\n",
      "7060\n",
      "7070\n",
      "7080\n",
      "7090\n",
      "7100\n",
      "7110\n",
      "7120\n",
      "7130\n",
      "7140\n",
      "7150\n",
      "7160\n",
      "7170\n",
      "7180\n",
      "7190\n",
      "7200\n",
      "7210\n",
      "7220\n",
      "7230\n",
      "7240\n",
      "7250\n",
      "7260\n",
      "7270\n",
      "7280\n",
      "7290\n",
      "7300\n",
      "7310\n",
      "7320\n",
      "7330\n",
      "7340\n",
      "7350\n",
      "7360\n",
      "7370\n",
      "7380\n",
      "7390\n",
      "7400\n",
      "7410\n",
      "7420\n",
      "7430\n",
      "7440\n",
      "7450\n",
      "7460\n",
      "7470\n",
      "7480\n",
      "7490\n",
      "7500\n",
      "7510\n",
      "7520\n",
      "7530\n",
      "7540\n",
      "7550\n",
      "7560\n",
      "7570\n",
      "7580\n",
      "7590\n",
      "7600\n",
      "7610\n",
      "7620\n",
      "7630\n",
      "7640\n",
      "7650\n",
      "7660\n",
      "7670\n",
      "7680\n",
      "7690\n",
      "7700\n",
      "7710\n",
      "7720\n",
      "7730\n",
      "7740\n",
      "7750\n",
      "7760\n",
      "7770\n",
      "7780\n",
      "7790\n",
      "7800\n",
      "7810\n",
      "7820\n",
      "7830\n",
      "7840\n",
      "7850\n",
      "7860\n",
      "7870\n",
      "7880\n",
      "7890\n",
      "7900\n",
      "7910\n",
      "7920\n",
      "7930\n",
      "7940\n",
      "7950\n",
      "7960\n",
      "7970\n",
      "7980\n",
      "7990\n",
      "8000\n",
      "8010\n",
      "8020\n",
      "8030\n",
      "8040\n",
      "8050\n",
      "8060\n",
      "8070\n",
      "8080\n",
      "8090\n",
      "8100\n",
      "8110\n",
      "8120\n",
      "8130\n",
      "8140\n",
      "8150\n",
      "8160\n",
      "8170\n",
      "8180\n",
      "8190\n",
      "8200\n",
      "8210\n",
      "8220\n",
      "8230\n",
      "8240\n",
      "8250\n",
      "8260\n",
      "8270\n",
      "8280\n",
      "8290\n",
      "8300\n",
      "8310\n",
      "8320\n",
      "8330\n",
      "8340\n",
      "8350\n",
      "8360\n",
      "8370\n",
      "8380\n",
      "8390\n",
      "8400\n",
      "8410\n",
      "8420\n",
      "8430\n",
      "8440\n",
      "8450\n",
      "8460\n",
      "8470\n",
      "8480\n",
      "8490\n",
      "8500\n",
      "8510\n",
      "8520\n",
      "8530\n",
      "8540\n",
      "8550\n",
      "8560\n",
      "8570\n",
      "8580\n",
      "8590\n",
      "8600\n",
      "8610\n",
      "8620\n",
      "8630\n",
      "8640\n",
      "8650\n",
      "8660\n",
      "8670\n",
      "8680\n",
      "8690\n",
      "8700\n",
      "8710\n",
      "8720\n",
      "8730\n",
      "8740\n",
      "8750\n",
      "8760\n",
      "8770\n",
      "8780\n",
      "8790\n",
      "8800\n",
      "8810\n",
      "8820\n",
      "8830\n",
      "8840\n",
      "8850\n",
      "8860\n",
      "8870\n",
      "8880\n",
      "8890\n",
      "8900\n",
      "8910\n",
      "8920\n",
      "8930\n",
      "8940\n",
      "8950\n",
      "8960\n",
      "8970\n",
      "8980\n",
      "8990\n",
      "9000\n",
      "9010\n",
      "9020\n",
      "9030\n",
      "9040\n",
      "9050\n",
      "9060\n",
      "9070\n",
      "9080\n",
      "9090\n",
      "9100\n",
      "9110\n",
      "9120\n",
      "9130\n",
      "9140\n",
      "9150\n",
      "9160\n",
      "9170\n",
      "9180\n",
      "9190\n",
      "9200\n",
      "9210\n",
      "9220\n",
      "9230\n",
      "9240\n",
      "9250\n",
      "9260\n",
      "9270\n",
      "9280\n",
      "9290\n",
      "9300\n",
      "9310\n",
      "9320\n",
      "9330\n",
      "9340\n",
      "9350\n",
      "9360\n",
      "9370\n",
      "9380\n",
      "9390\n",
      "9400\n",
      "9410\n",
      "9420\n",
      "9430\n",
      "9440\n",
      "9450\n",
      "9460\n",
      "9470\n",
      "9480\n",
      "9490\n",
      "9500\n",
      "9510\n",
      "9520\n",
      "9530\n",
      "9540\n",
      "9550\n",
      "9560\n",
      "9570\n",
      "9580\n",
      "9590\n",
      "9600\n",
      "9610\n",
      "9620\n",
      "9630\n",
      "9640\n",
      "9650\n",
      "9660\n",
      "9670\n",
      "9680\n",
      "9690\n",
      "9700\n",
      "9710\n",
      "9720\n",
      "9730\n",
      "9740\n",
      "9750\n",
      "9760\n",
      "9770\n",
      "9780\n",
      "9790\n",
      "9800\n",
      "9810\n",
      "9820\n",
      "9830\n",
      "9840\n",
      "9850\n",
      "9860\n",
      "9870\n",
      "9880\n",
      "9890\n",
      "9900\n",
      "9910\n",
      "9920\n",
      "9930\n",
      "9940\n",
      "9950\n",
      "9960\n",
      "9970\n",
      "9980\n",
      "9990\n",
      "10000\n",
      "10010\n",
      "10020\n",
      "10030\n",
      "10040\n",
      "10050\n",
      "10060\n",
      "10070\n",
      "10080\n",
      "10090\n",
      "10100\n",
      "10110\n",
      "10120\n",
      "10130\n",
      "10140\n",
      "10150\n",
      "10160\n",
      "10170\n",
      "10180\n",
      "10190\n",
      "10200\n",
      "10210\n",
      "10220\n",
      "10230\n",
      "10240\n",
      "10250\n",
      "10260\n",
      "10270\n",
      "10280\n",
      "10290\n",
      "10300\n",
      "10310\n",
      "10320\n",
      "10330\n",
      "10340\n",
      "10350\n",
      "10360\n",
      "10370\n",
      "10380\n",
      "10390\n",
      "10400\n",
      "10410\n",
      "10420\n",
      "10430\n",
      "10440\n",
      "10450\n",
      "10460\n",
      "10470\n",
      "10480\n",
      "10490\n",
      "10500\n",
      "10510\n",
      "10520\n",
      "10530\n",
      "10540\n",
      "10550\n",
      "10560\n",
      "10570\n",
      "10580\n",
      "10590\n",
      "10600\n",
      "10610\n",
      "10620\n",
      "10630\n",
      "10640\n",
      "10650\n",
      "10660\n",
      "10670\n",
      "10680\n",
      "10690\n",
      "10700\n",
      "10710\n",
      "10720\n",
      "10730\n",
      "10740\n",
      "10750\n",
      "10760\n",
      "10770\n",
      "10780\n",
      "10790\n",
      "10800\n",
      "10810\n",
      "10820\n",
      "10830\n",
      "10840\n",
      "10850\n",
      "10860\n",
      "10870\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(text)):\n",
    "#     text[i] = text[i].apply(correct_spelling)\n",
    "    text[i] = correct_spelling(text[i])\n",
    "    if i % 10 == 0:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "fundamental-concrete",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_with_spelling = text.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "tested-newman",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id\n",
       "1    [Our, Deeds, Reason, earthquake, May, ALL, For...\n",
       "4        [Forest, fire, near, La, Range, Task, Canada]\n",
       "5    [All, residents, asked, shelter, place, notifi...\n",
       "6    [13,000, people, receive, wildfires, evacuatio...\n",
       "7    [Just, got, sent, photo, Ruby, Alaska, smoke, ...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "healthy-emperor",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = text.apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "missing-retro",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id\n",
       "1           [deed, Reason, earthquake, ALLAH, forgive]\n",
       "4                   forest fire near Ronge Sask Canada\n",
       "5    resident shelter place notify officer evacuati...\n",
       "6    [13,000, people, receive, wildfire, evacuation...\n",
       "7    [send, photo, Ruby, Alaska, smoke, wildfires, ...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "turkish-metallic",
   "metadata": {},
   "source": [
    "### Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "visible-talent",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['there', 'forest', 'fire', 'spot', 'pond', 'geese', 'flee', 'across', 'street', 'save', 'they']\n"
     ]
    }
   ],
   "source": [
    "l = text[3]\n",
    "# l = train_data.text\n",
    "# doc = nlp()# running runner\")\n",
    "print([token.lemma_ for token in l])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "cathedral-thinking",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize(text):\n",
    "    return[token.lemma_ for token in text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "necessary-ticket",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatized_text = text.apply(lemmatize)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "raising-cheese",
   "metadata": {},
   "source": [
    "## From lemma back to token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "velvet-berkeley",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id\n",
       "1               [deed, Reason, earthquake, ALLAH, forgive]\n",
       "4                [forest, fire, near, Ronge, Sask, Canada]\n",
       "5        [resident, ask, shelter, place, notify, office...\n",
       "6        [13,000, people, receive, wildfire, evacuation...\n",
       "7        [send, photo, Ruby, Alaska, smoke, wildfires, ...\n",
       "                               ...                        \n",
       "10861    [earthquake, SAFETY, ANGELES, SAFETY, fastener...\n",
       "10865    [storm, bad, hurricane, city&amp;3other, hard,...\n",
       "10868                   [Green, Line, derailment, Chicago]\n",
       "10874                 [issue, Hazardous, Weather, Outlook]\n",
       "10875    [CityofCalgary, activate, Municipal, Emergency...\n",
       "Name: text, Length: 10876, dtype: object"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "legitimate-bhutan",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = text.apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "numeric-immune",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_to_tokens(text):\n",
    "    return [nlp(word) for word in text]\n",
    "#     token_list = []\n",
    "# #     text = nlp(text)\n",
    "#     for token in text:\n",
    "#         token_list.append(nlp(token)#.text)\n",
    "#     return token_list  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "owned-movement",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_text = text.apply(list_to_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "under-sweet",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id\n",
       "1    [(deed), (Reason), (earthquake), (ALLAH), (for...\n",
       "4    [(forest), (fire), (near), (Ronge), (Sask), (C...\n",
       "5    [(resident), (ask), (shelter), (place), (notif...\n",
       "6    [(13,000), (people), (receive), (wildfire), (e...\n",
       "7    [(send), (photo), (Ruby), (Alaska), (smoke), (...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_text.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "geographic-witch",
   "metadata": {},
   "source": [
    "## Vectorizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "id": "confident-birthday",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize(line):\n",
    "    return [word.vector for word in line]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 525,
   "id": "stretch-builder",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id\n",
       "1    [[-0.027740717, -0.11559065, 0.14615542, -0.11...\n",
       "4    [[3.0948722, 0.90669054, 0.21063933, -0.480993...\n",
       "5    [[-1.2147622, 0.49894887, 0.28072032, -0.30333...\n",
       "6    [[0.5988686, 2.0787547, 0.22975582, -0.5023366...\n",
       "7    [[-0.5375247, 1.4861925, -0.7751549, 0.3462769...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 525,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_text[0:5].apply(vectorize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 526,
   "id": "postal-calculator",
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_text = token_text.apply(vectorize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suited-violation",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "available-class",
   "metadata": {},
   "source": [
    "## Vectorization using TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "detailed-corpus",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# tfidf_vectorizer = TfidfVectorizer()\n",
    "# tfidf = str_text.apply(tfidf_vectorizer.fit_transform)\n",
    "# tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "subtle-official",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_to_str(l):\n",
    "    return ' '.join(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "anticipated-houston",
   "metadata": {},
   "outputs": [],
   "source": [
    "str_text = lemmatized_text.apply(list_to_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "intensive-level",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id\n",
       "1                deed Reason this earthquake ALLAH forgive\n",
       "4                       forest fire near Ronge Sask Canada\n",
       "5        resident ask shelter place be notify officer o...\n",
       "6        people receive wildfire evacuation order Calif...\n",
       "7        just send this photo from Ruby Alaska smoke fr...\n",
       "                               ...                        \n",
       "10861       earthquake SAFETY ANGELES SAFETY fastener XrWn\n",
       "10865    storm bad than last hurricane city&amp;3other ...\n",
       "10868                        Green Line derailment Chicago\n",
       "10874                      issue Hazardous Weather Outlook\n",
       "10875    CityofCalgary activate Municipal Emergency Pla...\n",
       "Name: text, Length: 10876, dtype: object"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "guilty-chick",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(str_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "supposed-palestine",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vect = TfidfVectorizer(analyzer='word', max_features=10000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "instrumental-completion",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vect.fit(str_text)\n",
    "tfidf = tfidf_vect.transform(str_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "coordinated-modeling",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<10876x10000 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 87344 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acute-philip",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "crucial-rogers",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "three-lover",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "just happen terrible crash   (0, 8246)\t0.2133565265618833\n",
      "  (0, 5819)\t0.36498017243543707\n",
      "  (0, 2882)\t0.46371919693647873\n",
      "  (0, 2432)\t0.34375279192218894\n",
      "  (0, 2107)\t0.5419728924830057\n",
      "  (0, 235)\t0.4408314582176946\n"
     ]
    }
   ],
   "source": [
    "print(str_text[0], tfidf[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "id": "employed-latex",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_vec = tfidf[0:len(train_data)]\n",
    "# test_vec = tfidf[len(train_data):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sustained-cross",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unusual-click",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "finnish-material",
   "metadata": {},
   "source": [
    "## Train-Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "supposed-afghanistan",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vec = tfidf[0:len(train_data)]\n",
    "test_vec = tfidf[len(train_data):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "imported-world",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = train_data.target\n",
    "X = train_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "endangered-discovery",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "strategic-machine",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "other-execution",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yulialysenko/opt/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:50:34] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
       "              importance_type='gain', interaction_constraints='',\n",
       "              learning_rate=0.300000012, max_delta_step=0, max_depth=6,\n",
       "              min_child_weight=1, missing=nan, monotone_constraints='()',\n",
       "              n_estimators=100, n_jobs=8, num_parallel_tree=1, random_state=0,\n",
       "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
       "              tree_method='exact', validate_parameters=1, verbosity=None)"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "xgb = XGBClassifier()\n",
    "xgb.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "unavailable-metro",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_xgb = xgb.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "subsequent-filename",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6929217668972859"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "f1_score(y_test,prediction_xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "handy-trustee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "neither-oxford",
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostClassifier\n",
    "\n",
    "clf = CatBoostClassifier(\n",
    "    #loss_function='CrossEntropy'\n",
    ")\n",
    "\n",
    "\n",
    "# clf.fit(X_train, y_train, \n",
    "\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "recreational-instruction",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'clf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-4f66baab8f93>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'clf' is not defined"
     ]
    }
   ],
   "source": [
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "union-locator",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "selective-teens",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
